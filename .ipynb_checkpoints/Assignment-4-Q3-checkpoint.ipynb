{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2046ca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54f0a68",
   "metadata": {},
   "source": [
    "# Problem 3) [4 pts] Application of Keras to build, compile, and train a neural network as a three class classifier for MNIST dataset (0 vs. 1 vs. 2):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3231b0f7",
   "metadata": {},
   "source": [
    "### a. Use mnist function in keras.datasets to load MNIST dataset and split it into training and testing sets. Then, randomly select 20% of the training images along with their corresponding labels to be the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3e6b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# randomly selection 20% images for validation\n",
    "max_range = len(x_train)\n",
    "random_list = random.sample(range(0, max_range), int(max_range * 20 / 100))\n",
    "\n",
    "x_val = np.asarray([x_train[i] for i in random_list])\n",
    "y_val = np.asarray([y_train[i] for i in random_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29898d6",
   "metadata": {},
   "source": [
    "### b. Feature extraction: average the pixel values in the quadrants in each image to generate a feature vector of 4 values for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79503f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_extract(images):\n",
    "  width=images.shape[1]\n",
    "  height=images.shape[2]\n",
    "  features=np.zeros((images.shape[0],4))\n",
    "  features_temp=np.sum(images[:,0:int(width/2),0:int(height/2)],axis=2)#quadrant 0\n",
    "  features[:,0]=np.sum(features_temp,axis=1)/(width*height/4)\n",
    "  features_temp=np.sum(images[:,0:int(width/2),int(height/2):],axis=2) #quadrant 1\n",
    "  features[:,1]=np.sum(features_temp,axis=1)/(width*height/4)\n",
    "  features_temp=np.sum(images[:,int(width/2):,0:int(height/2)],axis=2) #quadrant 2\n",
    "  features[:,2]=np.sum(features_temp,axis=1)/(width*height/4)\n",
    "  features_temp=np.sum(images[:,int(width/2):,int(height/2):],axis=2)  #quadrant 3\n",
    "  features[:,3]=np.sum(features_temp,axis=1)/(width*height/4)\n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847e1759",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the training, validation and testing feature (average of the four quadrants grid)\n",
    "feature_train=feat_extract(x_train)\n",
    "feature_val=feat_extract(x_val)\n",
    "feature_test=feat_extract(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6837d1",
   "metadata": {},
   "source": [
    "### c. Convert the label vectors for all the sets to binary class matrices using to_categorical() Keras function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b3a896",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "y_train_cat = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_cat = keras.utils.to_categorical(y_test, num_classes)\n",
    "y_val_cat = keras.utils.to_categorical(y_val, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b520d91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curve(accuracy_train, loss_train):\n",
    "  epochs=np.arange(loss_train.shape[0])\n",
    "  plt.subplot(1,2,1)\n",
    "  plt.plot(epochs,accuracy_train)\n",
    "  #plt.axis([-1,2,-1,2])\n",
    "  plt.xlabel('Epoch#')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.title('Training Accuracy')\n",
    "\n",
    "  plt.subplot(1,2,2)\n",
    "  plt.plot(epochs,loss_train)\n",
    "  plt.xlabel('Epoch#')\n",
    "  plt.ylabel('Binary crossentropy loss')\n",
    "  plt.title('Training loss')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaa27c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_report(title, model_in):\n",
    "    \n",
    "    print('----------------------------------------------------')\n",
    "    print(title)\n",
    "    print('----------------------------------------------------')\n",
    "    \n",
    "    history = model_in.fit(feature_train, y_train_cat, batch_size=16, epochs=50, verbose=0) \n",
    "    \n",
    "    train_score = model_in.evaluate(feature_train,y_train_cat)\n",
    "    \n",
    "    val_score = model_in.evaluate(feature_val, y_val_cat)\n",
    "    \n",
    "    test_score = model_in.evaluate(feature_test, y_test_cat)\n",
    "\n",
    "    print('Training Loss: ', train_score[0])\n",
    "    print('Training Accuracy: ', train_score[1])\n",
    "\n",
    "    print('Validation Loss: ', val_score[0])\n",
    "    print('Validation Accuracy: ', val_score[1])\n",
    "    \n",
    "    print('Test Loss: ', test_score[0])\n",
    "    print('Test Accuracy: ', test_score[1])\n",
    "    \n",
    "    plt.figure(figsize=[9,5])\n",
    "    acc_curve = np.array(history.history['accuracy'])\n",
    "    loss_curve = np.array(history.history['loss'])\n",
    "    plot_curve(acc_curve, loss_curve)\n",
    "    plt.show()\n",
    "    print('----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4486ff3",
   "metadata": {},
   "source": [
    "### d. Build, compile, train, and then evaluate:\n",
    "i. Build a neural network with 1 layer that contains 10 nodes using the Keras library. <br>\n",
    "ii. Compile the network. Make sure to select a correct loss function for this classification problem. Use stochastic gradient descent learning (SGD, learning rate of 0.0001). Explain your selection of the loss function.<br>\n",
    "iii. Train the network for 50 epochs and a batch size of 16.<br>\n",
    "iv. Plot the training loss (i.e., the learning curve) for all the epochs.<br>\n",
    "v. Use the evaluate() Keras function to find the training and validation loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b49d016",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = keras.Sequential()\n",
    "model1.add(keras.layers.Dense(input_dim=4, units=10, activation='softmax'))\n",
    "model1.summary()\n",
    "\n",
    "model1.compile(loss='categorical_crossentropy', \n",
    "               optimizer=tf.keras.optimizers.SGD(learning_rate=0.0001), \n",
    "               metrics=['accuracy'])\n",
    "\n",
    "model_report('Model 1 - 1 Layer, 10 nodes', model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1197a0a",
   "metadata": {},
   "source": [
    "### e. Repeat step (d) for each of the following networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe854c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.Sequential()\n",
    "model2.add(keras.layers.Dense(input_dim=4, units=50, activation='relu'))\n",
    "model2.add(keras.layers.Dense(units=10, activation='softmax'))\n",
    "model2.summary()\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy', \n",
    "               optimizer=tf.keras.optimizers.SGD(learning_rate=0.0001), \n",
    "               metrics=['accuracy'])\n",
    "\n",
    "model_report('Model 2 - 1 Layer, 50 nodes, and Output layer', model2, feature_train, y_train_cat, feature_val, y_val_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a404e5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = keras.Sequential()\n",
    "model3.add(keras.layers.Dense(input_dim=4, units=100, activation='relu'))\n",
    "model3.add(keras.layers.Dense(units=10, activation='softmax'))\n",
    "model3.summary()\n",
    "\n",
    "model3.compile(loss='categorical_crossentropy', \n",
    "               optimizer=tf.keras.optimizers.SGD(learning_rate=0.0001), \n",
    "               metrics=['accuracy'])\n",
    "\n",
    "model_report('Model 3 - 1 Layer, 100 nodes, and Output layer', model3, feature_train, y_train_cat, feature_val, y_val_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df44aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = keras.Sequential()\n",
    "model4.add(keras.layers.Dense(input_dim=4, units=100, activation='relu'))\n",
    "model4.add(keras.layers.Dense(units=10, activation='softmax'))\n",
    "model4.summary()\n",
    "\n",
    "model4.compile(loss='categorical_crossentropy', \n",
    "               optimizer=tf.keras.optimizers.SGD(learning_rate=0.0001), \n",
    "               metrics=['accuracy'])\n",
    "\n",
    "model_report('Model 4 : 2 Layers - 100 nodes, 10 nodes', model4, feature_train, y_train_cat, feature_val, y_val_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1c9a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = keras.Sequential()\n",
    "model5.add(keras.layers.Dense(input_dim=4, units=100, activation='relu'))\n",
    "model5.add(keras.layers.Dense(units=50, activation='relu'))\n",
    "model5.add(keras.layers.Dense(units=10, activation='softmax'))\n",
    "model5.summary()\n",
    "\n",
    "model5.compile(loss='categorical_crossentropy', \n",
    "               optimizer=tf.keras.optimizers.SGD(learning_rate=0.0001), \n",
    "               metrics=['accuracy'])\n",
    "\n",
    "model_report('Model 5 : 2 Layers - 100 nodes, 50 nodes, and Output layer', model5, feature_train, y_train_cat, feature_val, y_val_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3ead55",
   "metadata": {},
   "source": [
    "### f. What behavior do you observe in the training loss and the validation loss when you increase the number layers and nodes in the previous table. Which model is more suitable in this problem? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2399a180",
   "metadata": {},
   "source": [
    "### g. Evaluate the selected model in part (e) on the testing set and report the testing loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6801893d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
